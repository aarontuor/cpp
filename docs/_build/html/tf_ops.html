

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tf_ops &mdash; safekit 0.01 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="safekit 0.01 documentation" href="index.html"/>
        <link rel="next" title="np_ops" href="np_ops.html"/>
        <link rel="prev" title="About Safekit" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> safekit
          

          
            
            <img src="_static/eye.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.01
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="">tf_ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="np_ops.html">np_ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="batch.html">batch</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_training_utils.html">graph_training_utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">multivariate_dnn_autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">features</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">safekit</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>tf_ops</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/tf_ops.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-tf_ops">
<span id="tf-ops"></span><h1>tf_ops<a class="headerlink" href="#module-tf_ops" title="Permalink to this headline">¶</a></h1>
<p>Functions for building tensorflow computational graph models. RNN models,
and tensorflow loss functions will be added to this module.</p>
<dl class="function">
<dt id="tf_ops.batch_normalize">
<code class="descclassname">tf_ops.</code><code class="descname">batch_normalize</code><span class="sig-paren">(</span><em>tensor_in</em>, <em>epsilon=1e-05</em>, <em>decay=0.999</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#batch_normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.batch_normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch Normalization:
<a href="#id1"><span class="problematic" id="id2">`Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift`_</span></a></p>
<p>An exponential moving average of means and variances in calculated to estimate sample mean
and sample variance for evaluations. For testing pair placeholder is_training
with [0] in feed_dict. For training pair placeholder is_training
with [1] in feed_dict. Example:</p>
<p>Let <strong>train = 1</strong> for training and <strong>train = 0</strong> for evaluation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor_in</strong> &#8211; Input Tensor.</li>
<li><strong>epsilon</strong> &#8211; A float number to avoid being divided by 0.</li>
<li><strong>decay</strong> &#8211; For exponential decay estimate of running mean and variance.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tensor with variance bounded by a unit and mean of zero according to the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="tf_ops.diag_mvn_loss">
<code class="descclassname">tf_ops.</code><code class="descname">diag_mvn_loss</code><span class="sig-paren">(</span><em>truth</em>, <em>h</em>, <em>scale_range=1.0</em>, <em>variance_floor=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#diag_mvn_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.diag_mvn_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the output of a neural network after it&#8217;s last activation, performs an affine transform.
It returns the mahalonobis distances between the targets and the result of the affine transformation, according
to a parametrized Normal distribution with diagonal covariance. The log of the determinant of the parametrized
covariance matrix is meant to be minimized to avoid a trivial optimization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>truth</strong> &#8211; (tf.Tensor) The targets for this minibatch.</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>:param h:(tf.Tensor) The output of dnn.</dt>
<dd>(Here the output of dnn , h, is assumed to be the same dimension as truth)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>scale_range</strong> &#8211; For scaling the weight matrices (by default weights are initialized two 1/sqrt(fan_in)) for</td>
</tr>
</tbody>
</table>
<p>tanh activation and sqrt(2/fan_in) for relu activation.
:param variance_floor: (float, positive) To ensure model doesn&#8217;t find trivial optimization.
:return: (tf.Tensor, tf.Tensor) Loss matrix, log_of_determinants of covariance matrices.</p>
</dd></dl>

<dl class="function">
<dt id="tf_ops.dnn">
<code class="descclassname">tf_ops.</code><code class="descname">dnn</code><span class="sig-paren">(</span><em>x, layers=[100, 408], act=&lt;function relu&gt;, scale_range=1.0, bn=False, keep_prob=None, name='nnet'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#dnn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.dnn" title="Permalink to this definition">¶</a></dt>
<dd><p>An arbitrarily deep neural network. Output has non-linear activation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> &#8211; Input to the network.</li>
<li><strong>layers</strong> &#8211; List of sizes of network layers.</li>
<li><strong>act</strong> &#8211; Activation function to produce hidden layers of neural network.</li>
<li><strong>scale_range</strong> &#8211; Scaling factor for initial range of weights (Set to 1/sqrt(fan_in) for tanh, sqrt(2/fan_in) for relu.</li>
<li><strong>bn</strong> &#8211; Whether to use batch normalization.</li>
<li><strong>keep_prob</strong> &#8211; The percent of nodes to keep in dropout layers.</li>
<li><strong>name</strong> &#8211; For naming and variable scope.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">(tf.Tensor) Output of neural net. This will be just following a linear transform,
so that final activation has not been applied.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="tf_ops.dropout">
<code class="descclassname">tf_ops.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>tensor_in</em>, <em>prob</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds dropout node.
:param tensor_in: Input tensor.
:param prob: The percent of units to keep.
:return: Tensor of the same shape of <em>tensor_in</em>.</p>
</dd></dl>

<dl class="function">
<dt id="tf_ops.eyed_mvn_loss">
<code class="descclassname">tf_ops.</code><code class="descname">eyed_mvn_loss</code><span class="sig-paren">(</span><em>truth</em>, <em>h</em>, <em>scale_range=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#eyed_mvn_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.eyed_mvn_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>This function takes the output of a neural network after it&#8217;s last activation, performs an affine transform,
and returns the squared error of this result and the target.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>truth</strong> &#8211; A tensor of target vectors.</li>
<li><strong>h</strong> &#8211; The output of a neural network post activation.</li>
<li><strong>scale_range</strong> &#8211; For scaling the weight matrices (by default weights are initialized two 1/sqrt(fan_in)) for</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>tanh activation and sqrt(2/fan_in) for relu activation.
:return: (tf.Tensor, None) squared_error, None</p>
</dd></dl>

<dl class="function">
<dt id="tf_ops.full_mvn_loss">
<code class="descclassname">tf_ops.</code><code class="descname">full_mvn_loss</code><span class="sig-paren">(</span><em>truth</em>, <em>h</em>, <em>scale_range=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#full_mvn_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.full_mvn_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the output of a neural network after it&#8217;s last activation, performs an affine transform.
It returns the mahalonobis distances between the targets and the result of the affine transformation, according
to a parametrized Normal distribution. The log of the determinant of the parametrized
covariance matrix is meant to be minimized to avoid a trivial optimization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>truth</strong> &#8211; Actual datapoints to compare against learned distribution</li>
<li><strong>h</strong> &#8211; output of neural network (after last non-linear transform)</li>
<li><strong>scale_range</strong> &#8211; For scaling the weight matrices (by default weights are initialized two 1/sqrt(fan_in)) for</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>tanh activation and sqrt(2/fan_in) for relu activation.
:return: (tf.Tensor, tf.Tensor) Loss matrix, log_of_determinants of covariance matrices.</p>
</dd></dl>

<dl class="function">
<dt id="tf_ops.join_multivariate_inputs">
<code class="descclassname">tf_ops.</code><code class="descname">join_multivariate_inputs</code><span class="sig-paren">(</span><em>feature_spec</em>, <em>specs</em>, <em>embedding_ratio</em>, <em>max_embedding</em>, <em>min_embedding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#join_multivariate_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.join_multivariate_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes placeholders for all input data, performs a lookup on an embedding matrix for each categorical feature,
and concatenates the resulting real-valued vectors from individual features into a single vector for each data point in the batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feature_spec</strong> &#8211; A dict {categorical: [c1, c2, ..., cp], continuous:[f1, f2, ...,fk]
which lists which features to use as categorical and continuous inputs to the model.
c1, ..., cp, f1, ...,fk should match a key in specs.</li>
<li><strong>specs</strong> &#8211; A python dict containing information about which indices in the incoming data point correspond to which features.
Entries for continuous features list the indices for the feature, while entries for categorical features
contain a dictionary- {&#8216;index&#8217;: i, &#8216;num_classes&#8217;: c}, where i and c are the index into the datapoint, and number of distinct
categories for the category in question.</li>
<li><strong>embedding_ratio</strong> &#8211; Determines size of embedding vectors for each categorical feature: num_classes*embedding_ratio (within limits below)</li>
<li><strong>max_embedding</strong> &#8211; A limit on how large an embedding vector can be.</li>
<li><strong>min_embedding</strong> &#8211; A limit on how small an embedding vector can be.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple (x, placeholderdict):
(tensor with shape [None, Sum_of_lengths_of_all_continuous_feature_vecs_and_embedding_vecs],
dict to store tf placeholders to pair with data, )</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="tf_ops.multivariate_loss">
<code class="descclassname">tf_ops.</code><code class="descname">multivariate_loss</code><span class="sig-paren">(</span><em>h</em>, <em>loss_spec</em>, <em>placeholder_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#multivariate_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.multivariate_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a multivariate loss according to loss_spec.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>h</strong> &#8211; Final hidden layer of dnn or rnn. (Post-activation)</li>
<li><strong>loss_spec</strong> &#8211; <p>A tuple of 3-tuples of the form (input_name, loss_function, dimension) where
input_name is the same as a target in datadict,</p>
<blockquote>
<div>loss_function takes two parameters, a target and prediction,
and dimension is the dimension of the target.</div></blockquote>
</li>
<li><strong>placeholder_dict</strong> &#8211; A dictionary to store placeholder tensors for target values.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">(loss, loss_vector, loss_matrix, loss_names) where:
loss is a scalar value (average loss over loss contributors and mini-batch)
loss_vector is MB X 1 tensor of average loss per data point in mini-batch.
loss_matrix is MB X concatenated_feature_size tensor containing loss for all contributors for each data point.
loss_names is a list concatenated_feature_size long with names of all loss contributors.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="tf_ops.softmax_dist_loss">
<code class="descclassname">tf_ops.</code><code class="descname">softmax_dist_loss</code><span class="sig-paren">(</span><em>truth</em>, <em>h</em>, <em>dimension</em>, <em>scale_range=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tf_ops.html#softmax_dist_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tf_ops.softmax_dist_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>This function paired with a tensorflow optimizer is multinomial logistic regression.
It is designed for cotegorical predictions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>truth</strong> &#8211; A tensorflow vector tensor of integer class labels.</li>
<li><strong>h</strong> &#8211; A placeholder if doing simple multinomial logistic regression, or the output of some neural network.</li>
<li><strong>scale_range</strong> &#8211; For scaling the weight matrices (by default weights are initialized two 1/sqrt(fan_in)) for</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>tanh activation and sqrt(2/fan_in) for relu activation.
:return: Cross-entropy of true distribution vs. predicted distribution.</p>
</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="np_ops.html" class="btn btn-neutral float-right" title="np_ops" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="About Safekit" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, safekit_authors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.01',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>